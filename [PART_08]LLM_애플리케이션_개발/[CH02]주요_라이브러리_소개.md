## [CH08_02] 주요 라이브러리 소개

### 유명한 라이브러리
- LangChain
- LamaIndex
- Bursel AI SDK
- Guidance

## LangChain
- 언어 모델을 활용한 app을 빠르게 개발할 수 있도록
  - **언어 모델 어플리케이션 개발에 필요한 각종 기능을 통합하여 제공**하는 라이브러리

### LangChain Modules

#### LLM: 언어모델 Wrapper
- 여러가지 LLM을 동일한 인터페이스로 호출 가능
```python
from langchain.llms import OpenAI
llm = OpenAI(model_name="gpt-3.5-turbo", temperature=0)
llm("안녕, 세계!")
```

#### Prompt: 프롬프트 템플릿을 작성하고, 변수들을 조합하는 모듈
```python
from langchain import PromptTemplate

template = """
안녕, {target}!
"""

prompt = PromptTemplate(
  input_variables=["target"],
  template=template,
)

prompt.format(target="세계")
```

#### Memory: 대화 히스토리 구성 모듈
- 사용자의 히스토리를 저장하고 그것을 꺼내와서 사용
```python
from langchain.memory import ChatMessageHistory

history = ChatMessageHistory()

history.add_user_message("안녕, AI!")
history.add_ai_message("안녕, 인간!")

...

[HumanMessage(content='안녕, AI!', additional_kwargs={}),
AIMessage(content='안녕, 인간!', additional_kwargs={})]
```

#### Chain
- 여러가지 프롬프트를 체이닝
- 여러 LLM 호출을 연결하는 모듈
- 하나의 결과를 다른 결과로 연결하여
  - 복잡한 task를 수행할 수 있도록 하는 모듈
```python
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.chains import SimpleSequentialChain
from langchain prompts import PromptTemplate

llm = ChatOpenAI (model_name="gpt-3.5-turbo", temperature=0.7)

template="다음을 그대로 따라해 \n안녕, {target}!"

prompt_template = PromptTemplate(
  input_variables=["target"],
  template=template
)

# chain_1의 결과를 chain_2로 사용
chain_1 = LLMChain(llm=llm, prompt=prompt_template)
chain_2 = LLMChain(llm=llm, prompt=prompt_template)

chains = SimpleSequentialChain(
  chains=[chain_1, chain_2], 
  verbose=True
)

review = chains.run("세계")
```

#### Agents: 사용자 입력에 따라 미리 정해진 LLM/기타 도구 호출
- 사용자의 입력에 따라 달라지는 로직을 처리
- 여러 가지 기능들을 조합하여 쉽게 어플리케이션 만들기 가능
```python
from langchain.agents import load_tools
from langchain.agents import initialize_agent 
from langchain.agents import AgentType 
from langchain.llms import OpenAI
Ilm = OpenAI (temperature=0)
tools = load_tools(["llm-math"], llm=llm)
agent = initialize_agent(tools, llm,
  agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, 
  verbose=True)
agent.run("10 더하기 10은?")
```
- 미리 다양한 액션을 복합적으로 실행하는 에이전트를 만듦
- 툴을 사용한 에이전트라면, 특정한 툴을 여러개 넣어주고
  - 그 툴을 사용하여 결과 반환

#### Document Loaders, Text Splitter: 문서 파일 로딩 및 문서를 적절한 청크로 자름
- LLM Context 정보 제공을 위해 다양한 파일 로딩
```python
from langchain.document_loaders import DirectoryLoader 
from langchain. text_splitter import RecursiveCharacterTextSplitter

directory = '/content/data' # (.txt, pdf)

loader = DirectoryLoader(directory)
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)
docs = text_splitter.split_documents(documents)
```
- 문서를 로딩하고 문서를 적절한 청크로 자르는 것이
  - 어떤 경우에는 prompt를 만드는것보다 더 많은 시간이 소요될 수 있음
- 파일 형식이나 문서의 형식에 따라 공통 사용이 가능하기 때문에
  - 다양한 경우에 쉽게 사용할 수 있도록 랭체인 구성

#### Vector Stores: Vector DB Wrapper
```python
from langchain.embeddings.openai import OpenAIEmbeddings 
from langchain.vectorstores import Pinecone

embeddings = OpenAIEmbeddings(model_name="ada")

query_result = embeddings.embed_query("Hello world")

pinecone.init(
  api_key="<pinecone api key>",
  environment="<env>"
)

index_name = "langchain-test"

index = Pinecone.from_documents(docs, embeddings, index_name=index_name)
```
- pinecone이나 croma db 같은 것을이 Interation 되어 있음
- 여러가지 Vector DB를 동일한 인터페이스로 사용 가능

#### QA Chain with Vector Stores
```python
from langchain.llm import OpenAI 
from langchain.chains.question_answering import load_qa_chain

llm = OpenAI(model_name="text-davinci-003")
chain = load_qa_chain(llm, chain_type="stuff")
similar_docs = index.similarity_search(query, k=5)
query= "질문"
answer = chain.run(input_documents=similar_docs, question=query)
```
- 사용할 LLM과 Vector store만 잘 설정하면
  - 간단한 코드로 **질문 답변을 쉽게 만들어낼 수 있음**

### LangChain Problems
- 구조 복잡도에 비해 **문서가 부족, 구현이 깔끔하지 않음**
- 대부분의 모듈이 있는 타 라이브러리나 서비스 Wrapper에 불과하여
  - **간단한 작업을 복잡하게 만들고**
  - **프롬프트 구현이 숨겨져 있어 디버깅을 어렵게 만듦**
- 프로토타입 개발에는 유용하나, **프로덕션에 사용하기엔 지양함** 
- 실제 강사는
  - 프로토타입에서 활용하지 않음
  - LangChain 모듈 안에 있는 **구현**이나 **프롬프트**를 참고하기 위한 코드를 볼때 사용

## LlamaIndex

### LlamaIndex Modules

#### Data Connectors
- 사용자의 데이터 소스에서 데이터 로딩
- API, PDF, SQL 등 다양한 소스 지원
```python
from llama_index import download_loader
GoogleDocsReader = download_loader('GoogleDocsReader')
loader = GoogleDocsReader()
documents = loader.load_data(document_ids=[...])
```
- **Llama Hub**에서 다양한 데이터 소스를 확인할 수 있음
  - airtable, croma DB, confluence, discord, ...

#### Data Indexes
- LLM App에 주입할 데이터를 획득하기 위한 데이터를 **구조화**하는 모듈
- Vector Index뿐 아니라, Tree, List, Graph 등
  - 다양한 Indexing 방식 및 Query 전략을 지원
```python
from llama_index import VectorStoreIndex
index = VectorStoreIndex.from_documents(documents)
```
- 복잡한 데이터를 정밀하게 가져올 때 사용

#### Query Engines
- 인덱싱한 데이터에 대해 **질문**을 할 수 있는 모듈
```python
query_engine = index.as_query_engine()
response = query_engine.query("LLM이 뭐야?")
```
- LangChain의 **QE Chain**과 동일한 역할

#### Chat Engines
- Multi-turn으로 질문할 수 있는 인터페이스 지원 모듈
```python
chat_engine = index.as_chat_engine()
response1 = chat_engine.chat("질문")
response2 = chat_engine.chat("후속 질문")
```
- LangChain의 **Chain**과 같은 역할

#### Data Agent
- 외부 서비스 API를 호출해 **데이터를 동적으로 수집**하는 모듈
```python
from llama_hub.tools.wikipedia.base import WikipediaToolSpec 
from llama_index.agent import OpenAIAgent

tool_spec = WikipediaToolSpec()

agent = OpenAIAgent.from_tools(tool_spec.to_tool_list())
agent.chat('Who is Ben Afflecks spouse?')
```
- 위키피디아 검색, 구글 검색, 열려 있는 Open API를 통해 데이터를 가져옴
- Data Agent의 경우에도 **Llama Hub**에 가보면 다양한 것들이 존재

### 정리
- Llama Index는 RAG 관점에서 잘 포커싱된 라이브러리
  - 컨텍스트 주입에 유용하게 사용 가능
  - RAG: Retrieval-Argumented Generation(검색 증강 생성)
- **데이터 핸들링에는 Llama Index를 추천**
- **프롬프트 체인은 직접 구축**

## Guidance
- 구조화된 텍스트 템플릿을 사용해서 **프롬프트를 구조적으로 작성**
- 지시문과 출력을 **효율적**으로 제어
- 언어 모델이 텍스트르를 실제로 처리하는 방식과 일치하게 생성
- 프롬프트, 논리적 제어를 **단일 연속 흐름**으로 작성 가능
- https://github.com/guidance-ai/guidance

### Guidance Template
```python
import guidance
guidance.llm = guidance.llms.OpenAI("gpt-3.5-turbo")
program = guidance('''
{{#system~}}
당신은 유능한 AI 비서입니다.
{{~/system}}

{{#user~}}
다음 질문에 답하세요.
{{query}}
세 문단으로 답하세요.
{{~/user}}

{{#assistant~}}
{{gen 'answer' temperature=0 max_tokens=256}}
{{~/assistant}}''')

program(query='LLM이 뭐니?')
```

## Vercel AI SDK
- 언어 모델을 활용한 **웹 어플리케이션**을 쉽게 만들 수 있도록 도와주는 라이브러리
- **스트리밍**을 사용한 대화형 사용자 인터페이스를 쉽게 만들 수 있음
- 실습 app에 사용 예정
- https://sdk.vercel.ai/docs

## 차후 실습 방향
- Vercel AI SDK외에 다른 라이브러리는 사용하지 x
- 추상화된 라이브러리를 사용하지 않고,
  - 최대한 RAW하게 구현하여 개념/원리 파악
