## [CH07_04] 스트리밍 응답 처리
- GPT의 응답 반환 속도가 느림에 따라, 이를 극복하기 위한 트릭

### 스트리밍 응답
- 생성 결과 전체를 한 번에 응답 받는 것이 아닌
  - **토큰을 생성할 때마다 응답을 전송 받는 방법**
- 순차적으로 단어가 생성되며 보여짐
  - GPT가 조금 더 사람 처럼 보이게 됨

### 일반 응답
- content에 따라 한번에 응답을 받아옴
```json
{
  "id": "chatcmp1-7g8Tp6ie7t5qSoWzzcgSCG3RqfFD3",
  "object": "chat.completion",
  "created": 1690276729,
  "model": "gpt-3.5-turbo-0613",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "안녕? Hello?"
      },
      "finish reason": "stop"
    }    
  ],
  "usage": {
    "prompt_tokens" : 20,
    "completion tokens": 7,
    "total tokens": 27
  }
}
```

#### 스트리밍 응답
- 토큰 단위로 답변이 생성됨
```json
data: {"id": "chatcmp1-7g8SCK7mjlAT8×FaIX0shDvmV903n", "object": "chat.completion.chunk", "created": 1690276628, "model": "got-3.5-turbo-0613", "choices": [{"index" : 0, "delta": {"role": "assistant", "content": ""}, "finish reason": null}]}
data: {"id": "chatcmp1-7g8SCK7mjlAT8xFaIX0shDvmV903n", "object": "chat.completion.chunk", "created": 1690276628, "model": "gpt-3.5-turbo-0613", "choices": [{"index" : 0, "delta": {"content": "안"}, "finish_reason": null}]}
data: {"id": "chatcmp1-7g8SCK7mjlAT8xFaIXOshDvmV903n", "object": "chat.completion. chunk", "created": 1690276628, "model": "gpt-3.5-turbo-0613", "choices": [{"index" : 0, "delta": {"content": "녕"}, "finish_reason": null}]}
data: {"id": "chatcmp1-7g8SCK7mj1AT8xFaIX0shDvmV903n", "object": "chat.completion.chunk", "created": 1690276628, "model": "gpt-3.5-turbo-0613", "choices": [{"index" : 0, "delta": {"content": "?"}, "finish_reason": null}]}
...

data: {"id": "chatcmp1-7g8SCK7mj 1AT8×FaIX0shDvmV903n", "object": "chat.completion.chunk", "created": 1690276628, " model": "gpt-3.5-turbo-0613", "choices": [{"index": 0, "delta": {},"finish_reason":"stop"}]}

data: [DONE]
```
- 한글이기 때문에 한 글자씩 보이나,
  - token 단위 반환임
- 마지막에는 `DONE`으로 스트리밍 응답이 끝남

### 토큰 수 계산에서의 차이
- 일반 응답과 스트리밍 응답에서의 차이는 존재함
- 단, 스트리밍 결과 때문에 달라지는 것이 아닌
  - jupyti의 tokenizer의 차이때문에 발생하기도 하며
  - 스트리밍 방식에 따라 차이가 발생하기도 함
- **필요한 토큰 수가 있을때는 꼭 계산해서 확인할 것**

### 스트리밍 응답의 특징
- 토큰이 생성될 때마다 사용자에게 표시함으로써
  - 생성 속도가 느린 **LLM의 단점을 상쇄**하고 인터렉티브한 경험 제공
- 생성 중간에 생성 결과를 평가하여 **중단**, **분기** 가능
- `timeout` 처리를 조금 더 유연하게 할 수 있음
- 생성한 텍스트 결과만 토큰 단위로 등답
  - **전체 응답에 대한 부가 정보를 응답에 포함하지 않음**
    - e.g. 입출력 토큰 수, 전체 토큰 수, 생성 시간, 세션 정보
- 응답에 대한 로그를 남기는 작업이 번거로울 수 있음
- 스트리밍 응답에 대한 **핸들링** 및 **추가적인 예외 상황**에 대한 처리 필요
  - 필요한 통신 구간 전체를 **스트리밍 파이프라인**으로 구추갷야 함
- 전송되는 **패킷 사이즈**가 크게 증가(네트워크 사용량 증가)

### 결론
- **전처리 구간**과 **최종 응답 구간**을 나누고
  - **최종 응답 구간**에만 **스트리밍 응답**으로 사용하는 것을 권장
