## [CH06_04] 임베딩과 시멘틱 서치

### Grounding
- LLMd에게 검색 등을 통해 문맥에 맞는 정보를 제공하거나
  - 계산기등을 사용해 정확한 계산 결과를 제공하는 방법
- 정확한 정보를 제공하여 환각을 줄이거나
  - LLM 학습 후 나온 새로운 정보를 이용한 결과를 생성할 수 있음

### RAG
- Retrieval-Argumented Generation
- 정보를 검색해와서 이를 기반으로 **텍스트**를 생성하는 방법
- 대화의 **맥락**을 유지하기 위한 **장기 기억 메모리**로도 활용
- 예시
  - 사용자가 질문을 하면
  - 이 정보를 검색이나 벡터 db에서 가져옴
  - 이를 활용하여 답변

### Embedding
- 텍스트를 언어모델에 넣어서
  - 벡터(숫자들의 모음)으로 만드는 것
- 각 단어, 문장마다 Embedding vector

### Vector Search
- 벡터간의 거리를 계산
- 가장 가까운 거리의 텍스트를 찾아냄
- Distance, Similarity
- 벡터들의 유사도
  - 텍스트의 유사도
- Semantic Search

### Embedding Model
- 해당 모델들도 생성 모델 처럼 성능 비교 리더보드 들이 많음
- 어떤 모델을 사용할지 선택할때
  - **Retrieval 성능을 보고 판별할 것**
- Retrieval 성능이 같더라도
  - 세부 task마다 성능이 다르기 때문에
  - 필요한 task를 확인할 것

### 임베딩 모델 선택
- 모델의 성능은 데이터셋마다 다름
  - **내가 사용할 데이터에 대해 테스트를 하고, 가장 적절한 모델을 고르기**
- 민감하거나 정확도가 높아야하는 데이터의 경우
  - **파인 튜닝한 모델**을 사용하는 방법 고려
- OpenAI의 `text-embedding-ada-002` 모델이 대부분의 경우
  - 일정 수준 이상의 결과를 보여주며, 한국어에서도 대체로 성능이 우수함
  - 가격도 저렴하여 기본 선택 모델로 좋은 편
  
### Vector DB
- Vector Search를 통해 가장 가까운 벡터를 찾아주고
  - 그 찾아온 벡터가 **어떤 문서의 벡터**인지 알 수 있도록 메타정보를 함께 반환

#### Vector DB의 기능
- 메타 데이터와 함께 결과 반환
- 필터링 등을 이용한 하이브리드 검색
- 실시간 인덱싱
- 다양한 인덱싱 및 검색 알고리즘 제공
- 높은 확장성 및 편의 기능 등

#### Vector DB로 유명한 서비스
- **Pincone**
  - 가장 유명함
  - 유일하게 오픈소스가 아님
  - 본격적인 서비스시 활용
  - 클라우드 서비스만 제공(설치형 x)
  - On-premise를 구축하려면 Milvus, Weaviate, Qdrant 참고
- Milvus
- Weaviate
- Qdrant
- **Chroma**
  - 최근에 나온 db
  - 요즘 대세
  - 간단하게 쓰기 좋음
- 기존 db에서도 Vector Search를 지원하기 시작
  - Redis
  - Elasticsearch
  - PostgreSQL

#### Vector DB 선택시 유의사항
- 속도와 정확도(성능)은 trade-off
- 자신의 서비스에 어떤게 맞는지 잘 판단하여 선택

### Hybrid Search
- 검색의 정확도를 높히기 위해
  - **키워드 필터링**이나 `Dense, Sparse Vector`등을 조합하여 검색
- e.g. 키워드 검색으로 먼저 전체문서에서 키워드에 맞는 일부 문서 검색
  - 그 문서들 중 `Vector Search`로 가장 가까운 문서를 찾음

### Re-rank
- LLM에 많은 검색 결과를 넣으면 비용, 속도상에 문제 발생 가능성 존재
  - token 제한에 걸릴 수 있음
- 이를 극복하기 위해 Re-rank 기법 사용
- **1차 Vector Search**에 사용한 것과 다른 경량의 임베딩 모델을 사용하거나
- 1차 결과를 **LLM**으로 사용자 요청에 맞게 **재정렬**하여 생성하는 등
  - Re-rank 목적에 맞는 방식 사용

### Chunking
- 텍스트를 **적절한 길이**로 자르는 방법
- **임베딩 모델**의 **최대 토큰 수**를 넘는 텍스트를 임베딩하기 위해 사용
- 긴 텍스트를 여러개의 작은 부분으로 나우어
  - 각 부분을 독립적으로 임베딩할 수 있게 하여
  - **텍스트 데이터의 특징을 더 잘 표현하고**
    - 더욱 정교하게 검색하거나 분석하기 위해 사용
- **문장**단위로 자르거나 **문단**단위로 자르기도 함
- 단순히 토큰 수 단위나 단어 단위로 잘라도 되지만
  - **문장**이나 **문단**, **구조화된 문서**라면 **섹션 단위**로 자르는 방법 사용
- 임베딩 모델과 문서의 성격에 따라 **적절한 청킹 방법**이 다름
  - `문서에 따라 실험을 통해` 적절한 방법과 길이의 청킹 방법을 찾아야 함
- OpenAI의 `text-embedding-ada-002` 모델의 경우
  - 일반적으로 `200 ~ 500` 토큰 사이의 길이 추천

### Overlap & Sliding
- 텍스트를 분리했을 때 의미가 소실되거나 왜곡되는 것을 방지하고
  - **문맥을 보존**하기 위해 사용

#### Overlap
- 각 청크가 일부의 **공통된 데이터**를 포함하도록 하는 기법
- e.g. 3개의 문단이 있다고 할때, 2개의 chunk로 분리
  - 1,2문단, 2,3문단

#### Sliding Window
- 일정한 길이의 토큰(혹은 단어 등) 윈도우로 텍스트를 슬라이드 하면서
  - 데이터의 청크를 캡쳐하는 방법
- Context Window와 유사
- Window Size, Overlap size를 가지고 진행
  - `Window Size`만큼 Chunk의 크기가 결정되며
  - `Overlap size`만큼 겹치는 크기가 결정됨

### 결론
- 청킹 전략과 벡터 서치 방법, 하이브리드 서치 방법을 다양하게 연구해보면 좋음
