## [CH06_03] 프롬프트 평가와 테스팅
- 이 강의의 가장 중요한 파트중 하나

### 좋은 프롬프트 만들기 Remind
- 항상 같은 방식으로 동일한 결과를 보장할 수 없음
- 반복적으로 개선하는 전략이 필요
- 광범위한 시행착오를 필요로 함
- 한 번 그럴듯한 결과를 내는 것이 아닌
  - 원하는 결과를 정확히 의도한대로 항상 일관성 있게 내도록 만드는 것
- 소프트웨어는 만드는 기간보다 **유지보수 하는 시간**이 훨씬 더 길다
- **프롬프트 엔지니어링 = 평가를 반복하며 프롬프트를 만드는 일련의 작업**

### 소프트웨어 요구사항 명세
- Software Requirements Specification
- 개발할 소프트웨어 제품이 어떻게 기능해야하는지에 대한 문서
- 프로그램이 처리해야할 **기능을 구체적으로 정의**하고
- **입출력 요건과 프로그래밍에 필요한 논리 개념을 정리**하는 문서
- 구체적인 시스템 **설계 단계 이전에 요구사항을 평가하는 것**
  - 나중에 재설계하는 것을 줄이는 것이 목표
  - 요구사항 사양을 적절히 활용하면 소프트웨어 프로젝트 실패 방지 가능
- 프롬프트 역시 입출력 조건을 정의하는 것이 중요

### LLM
- Prompt Engineering
- 답변을 위해 필요한 컨텍스트 제공 <- **컨텍스트 데이터**
- 원하는 결과 추출을 위한 프롬프트 작성 <- **Instruction or 사용자 입력 데이터**
- 원하는 포맷의 출력을 위한 프롬프트 작성 -> **출력 데이터**

### 프롬프트 요구사항 명세
- 컨텍스트 데이터
- 인스트럭션 or 사용자 입력 데이터
- 출력 데이터

#### 프롬프트 요구사항 명세 w/ Context
```md
# Context
마감된 공고는 아래의 경로를 통해 확인 가능합니다.
1. 원티드 채용 솔루션 [채용공고 관리]
클릭
2. 상단 [채용종료] 를 클릭하면 마감된 공고를 확인하실 수 있습니다.
* 마감된 공고를 다시 게시 원하실 경우, 게시 설정에서 원티드 공고 게시 ON, 게시 마감일을 다시 설정해주시면 됩니다

# User Input
마감된 공고는 어디서 확인이 가능한가요?

# Output
마감된 공고는 [채용공고 관리] > [채용종료] 탭을 통해 확인하실 수 있습니다
```
- 단 위와 같은 프롬프트 요구사항 명세는 1가지 이므로
  - 많은 데이터가 필요함

### 머신러닝 데이터 분류
- Traning: 60%
  - Models learn the task
- Validation: 20%
  - Which model is best?
- Test: 20%
  - How good is this model truly?
- 프롬프트 소프트웨어는 이미 학습이 끝난 LLM 모델 위에서 돌아가는 것이기 때문에
  - **테스트 데이터**만 필요함
- 그렇기 때문에 **ML 소프트웨어**보다 개발주기나 업데이트 주기가 빠름

### 필요한 테스트 셋의 크기
- OpenAI: https://platform.openai.com/docs/guides/gpt-best-practices/strategy-test-changes-systematically
- ![image](https://github.com/Wshid/dl_ml/assets/10006290/b46b408c-68d5-4e1c-9010-0eba7a43dbb2)
- 100개정도의 샘플이면, 어느정도는 실사용시 문제 없을만한 평가 가능
- **100개정도의 샘플에, 샘플의 다양성이 충분하기만 하면 됨**
- GPT에 경우 그렇다는 의미이며,
  - 다른 모델을 쓸 경우 어느정도 수준이 적절한 샘플 사이즈인지 확인은 필요함

### 프롬프트 버전 관리
- 실서비스에 나간 뒤 문제가 생겨 롤백이 필요한 경우
- LLM 모델이 변경되어 프롬프트 재탐색이 필요한 경우
- 변경사항 추적이 필요한 경우

### Versoning
- **X, Y**
  - X: Major Version
  - Y: Minor Version
- **Major Version 변경**
  - 출력 포맷의 변경
  - 출력 내용이나 구성이 많이 변경되는 경우
- **Minor Version**
  - 결과를 조금 더 정확하게 출력하도록 개선
  - 생성 옵션이 변경되는 경우

### 생성 결과 평가하기
- Extract matching
- 예시 데이터와 생성 결과의 임베딩 유사도 평가
- 인간 평가
- 생성 모델로 평가

#### 경험상 효과적인 부분
- 스펙과 정확한 값을 출력 - x
- 생성 결과와 예시 데이터의 인베이팅 값이 어느정도 유사한지 - x
- 사람이 직접 평가한 것 - o
- 생성 모델이 평가한 것 - o
- **얼마나 사람과 생성 모델이 유사하게 평가하는가를 가지고 성능 평가**
  - 다음 파트에서, 스프레드시트와 인간 평과와 생성모델평가를 함께 사용하는 방법 실습 예정

### 생성 결과 평가하기
1. **Google Sheets**
   - 사용이 간편함
   - 누구나 쉽게 접근 가능, 사용 가능
   - 협업하기 용이
   - 버전에 따른 관리 용이
   - 프로그램 통합 용이
   - 무료
   - 수많은 QA 툴이 있는데도 많이 사용되는 것을 보면...
2. https://github.com/openai/evals
3. https://github.com/langchain-ai/auto-evaluator
4. https://www.trulens.org
5. https://wandb.ai/site/prompts
6. ..