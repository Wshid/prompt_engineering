## [CH02_04] 어떻게 가능해졌나

### Autoregressive Model
- GPT와 같은 모델
- 특정 단어들을 주면, 후위에 올 단어를 예측
  - 여러가지 단어들 중 가장 확률이 높은 것을 리턴
- 후위에 온 단어를 입력으로 계산
- 똑같이 그 다음 올 단어를 확률화, 가장 높은 것을 리턴

### Transformer
- Autogressive Model을 활용한 기술
- 여러가지 텍스트가 순서대로 나열되어 있을 때
  - 그 다음 단어가 어떤 단어에 집중하는지 판단
- 모델을 키우면 키울수록 기능이 점차 확장됨
  - **감정 분류 기능**이 가능해짐
  - **요약 기능**
  - **번역 기능**

### Context Length
- 모든 기업이 집중하는 부분
- LSTM > BERT > GPT-3 > GPT-3.5 > GPT-4 > GPT-4 32K
  - LSTM: 약 한 문단
  - BERT부터 Transformer가 적용되면서 급격히 길어지기 시작
  - GPT-4: 약 12p
  - GPT-4 32K: 약 50p
  - Claude 100K: 약 150p
    - 왠만한 보험 약관도 제공 가능

### 코드 데이터 학습
- 추론 능력 향상
- 정형 데이터 생성 가능

### Instruction Tuning
- 명령/지시로 표현된 텍스트를 이해하여, 지시한 작업을 수행 
- chatbot 및 app 개발이 가능해짐

### RLHF
- Reinforce Learning from Human Feedback
- 성능 향상
- Alignment
  - 사람이 보기에 좋은, 윤리성 등 인간의 기준에 무해하도록
- 대량의 신규 데이터 자동 생성
- GPT A의 결과를 사람이 평가
- 이 평가를 기준으로 LLM 모델을 학습
- 텍스트를 생성하는 기술. 그리고 다시 학습
  - 자가 학습이 가능함

### Multimodal
- 컨텍스트 능력 극대화
- 텍스트뿐 아니라 이미지도 동시 학습하게 됨